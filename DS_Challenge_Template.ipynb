{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import linear_model, ensemble, tree\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import cross_val_score,GridSearchCV,cross_validate\n",
    "from sklearn.metrics import confusion_matrix, auc,roc_auc_score,roc_curve,recall_score,classification_report\n",
    "pd.set_option('display.max_rows', 100, 'display.max_columns', 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and Explore the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 1 :- Load the data\n",
    "Step 2 :- Print Number of rows and Columns\n",
    "Step 3 :- Print datatypes of all the columns in the data\n",
    "Step 4 :- Check for the duplicate rows in the data and remove them\n",
    "Step 5 :- Check for the missing values in the data \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def Load_File(path) :\n",
    "    if os.path.isfile(path) :\n",
    "        data = pd.read_csv(path)\n",
    "        print(\"\\n\" + \"Number of rows in data are %s\" % len(data))\n",
    "        print(\"Number of columns in data are %s\" % len(data.columns) + \"\\n\")\n",
    "        print(\"Following are the data types of columns:- \")\n",
    "        print(data.dtypes)\n",
    "        print(\"Data Import is Complete\")\n",
    "        print(data.isnull().sum())\n",
    "        return data\n",
    "    else:\n",
    "        print(path + \" does not exist. Enter the correct path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Check if there is imbalance in the data\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def overfit_columns(data, column_name):\n",
    "    \n",
    "    max_value = data[column_name].value_counts()[0]\n",
    "    total_sum = data[column_name].value_counts().sum()\n",
    "    \n",
    "    if (max_value/total_sum)*100 > 90:\n",
    "        print('%s feature categories are imbalanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Impute the missing values. \n",
    "* For Numerical Data use mean or median to impute the data.\n",
    "* For Categorical Data use mode to impute the data \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def categorical_imputing(df, column_name):\n",
    "    df[column_name] = df[column_name].fillna(df[column_name].mode().iloc[0])\n",
    "    print(df[column_name].unique())\n",
    "    return df\n",
    "\n",
    "def numerical_imputing_and_encoding(df, column_name):\n",
    "    df[column_name] = df[column_name].fillna(df.median())\n",
    "    df[column_name] = (df[column_name]-df[column_name].mean())/df[column_name].std()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Make Two Lists\n",
    "1. One for columns whose datatype is object\n",
    "2. Second for columns whose datatype is numeric\n",
    "\"\"\"\n",
    "\n",
    "categorical_columns = [column for column in data.columns if data[column].dtype == 'object']\n",
    "numerical_columns = [column for column in data.columns if data[column].dtype != 'object']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Make a barplot for object columns and see the counts of categories\"\"\"\n",
    "\n",
    "def barplot_count(data, Column_Name, size=(4,5), width = 0.25, height = 10., hue = None):\n",
    "    plt.figure(figsize=size)\n",
    "    plt.title('Count of %s Variable' % (Column_Name))\n",
    "    palette = sns.color_palette(\"husl\")\n",
    "    if not hue:\n",
    "        ax = sns.countplot(Column_Name, data=data, palette=palette, order = data[Column_Name].value_counts().index)\n",
    "    else:\n",
    "        ax = sns.countplot(Column_Name, data=data, palette=palette, hue=hue, order = data[Column_Name].value_counts().index)\n",
    "    \n",
    "    total = len(data[Column_Name])\n",
    "    for p in ax.patches:\n",
    "        height_bar = np.nan_to_num(p.get_height(), 0)\n",
    "        ax.annotate(height_bar, (p.get_x() + width, p.get_height()+height), fontsize=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"Plot For all the categorical columns \"\"\"\n",
    "\n",
    "# fig, ax = plt.subplots(1, len(categorical_columns), figsize=(25, 7))\n",
    "# for i, column in enumerate(categorical_columns):\n",
    "#     if len(data[column].unique()) <= 10:\n",
    "#         sns.countplot(column, data=data, palette=sns.color_palette(\"husl\"),\n",
    "#                       order = data[column].value_counts().index)\n",
    "\n",
    "# fig.show()\n",
    "\n",
    "fig,axs= plt.subplots(2,1,figsize=(8,12))\n",
    "fig.subplots_adjust(hspace=0.6)\n",
    "\n",
    "for i,ax in zip(categorical_columns,axs.flatten()):\n",
    "    sns.countplot(i, data=data, palette='husl', order = data[i].value_counts().index, ax=ax)\n",
    "    plt.xlabel(i,fontsize=12)\n",
    "    ax.set_title(str(i), fontweight='bold', size=20)\n",
    "    \n",
    "    total = len(data[i])\n",
    "    height = int(total*0.005)\n",
    "    print(total)\n",
    "    for p in ax.patches:\n",
    "        height_bar = np.nan_to_num(p.get_height(), 0)\n",
    "        ax.annotate(height_bar, (p.get_x() + 0.20, p.get_height()+height), fontsize=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Plot Dist plot and see which variables are skewed. Then we can use some transformation to make it less skewed\n",
    "For example :- Log Transformations, Square Transformation\n",
    "\"\"\"\n",
    "\n",
    "fig, ax = plt.subplots(1, len(numerical_columns), figsize=(25, 7))\n",
    "for i, column in enumerate(numerical_columns):\n",
    "    sns.distplot(data[column], ax=ax[i], kde=True, hist=False, norm_hist=True)\n",
    "    stats.probplot(data[column], plot=plt)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "data[column] = np.log10(data[col]+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Next Step is to CHECK FOR OUTLIERS in the data\"\"\"\n",
    "\n",
    "def drop_outliers(data, column):\n",
    "    iqr = 1.5 * (np.percentile(data[column], 75) - np.percentile(data[column], 25))\n",
    "    data.drop(data[data[column] > (iqr + np.percentile(data[column], 75))].index, inplace=True)\n",
    "    data.drop(data[data[column] < (np.percentile(data[column], 25) - iqr)].index, inplace=True)\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"NEXT STEPS IS ONE-HOT ENCODING OF THE DATA\"\"\"\n",
    "\n",
    "def categorical_encoding(df, column_name):\n",
    "    print(df[column_name].unique())\n",
    "    categorical_columns = pd.get_dummies(df[column_name], prefix = column_name, prefix_sep = '_', drop_first = True)\n",
    "    df = pd.concat([df, categorical_columns], axis = 1)\n",
    "    df = df.drop(column_name, axis = 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"STANDARDIZED THE DATASET FOR LOGISTIC REGRESSION\"\"\"\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "rob_scaler = RobustScaler()\n",
    "\n",
    "sns.kdeplot(np.log(std_scaler.fit_transform(data['amount'].values.reshape(-1,1))+1), bw=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. NEXT STEP IS TO CREATE FEATURE FROM THE DATA. FOR EXAMPLE :- WEEK FROM DATE, MONTH FROM DATE, ETC\n",
    "2. THEN WE CAN PLOT A CORRELATION HEATMAP TO SEE THE COLUMNS WHICH ARE HIGHLY RELATED. WE CAN REMOVE COLUMN WHICH ARE \n",
    "HIGHLY CORRELATED\n",
    "3. WE CAN ALSO USE VIF (VARIANCE INFLATION FACTOR) FOR THE COLUMNS\n",
    "4. THEN WE CAN REMOVE THE UNWANTED COLUMNS FROM THE DATA\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = final_data.drop([column_name], axis = 1)\n",
    "target  = final_data[target_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Split the data into training and testing data\n",
    "If the data is imbalanced then use SMOTE to balance the data\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(feature, target, \n",
    "                                                    test_size = 0.3, \n",
    "                                                    random_state = 0, \n",
    "                                                    stratify=target)\n",
    "\n",
    "\n",
    "smote = SMOTE()\n",
    "X_Smote_Train, Y_Smote_Train = smote.fit_sample(X_train, Y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Use Logistic Regression Model for Classification using Class Weights \"\"\"\n",
    "\n",
    "logistic_class_weights = {0:10, 1:100} \n",
    "logistic_regression_smote = LogisticRegression(random_state=12, class_weight=logistic_class_weights)\n",
    "logistic_regression_smote.fit(X_Smote_Train, Y_Smote_Train)\n",
    "\n",
    "\n",
    "coefficients = pd.concat([pd.DataFrame(X_Smote_Train.columns),\n",
    "                          pd.DataFrame(np.transpose(logistic_regression_smote.coef_))], axis = 1)\n",
    "coefficients.columns = ['Features', 'Coeff']\n",
    "coefficients['odds'] = np.exp(coefficients['Coeff'])\n",
    "coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Use K-Fold to built model\"\"\"\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, KFold\n",
    "cv = KFold(shuffle=True, random_state=2, n_splits=3)\n",
    "scores = cross_val_score(logistic_regression_smote, X_Smote_Train, Y_Smote_Train ,cv = cv, scoring = 'roc_auc')\n",
    "\n",
    "\n",
    "def printing_Kfold_scores(x_train_data,y_train_data):\n",
    "    \n",
    "    fold = KFold(shuffle=False, random_state=2, n_splits=3)\n",
    "\n",
    "    # Different C parameters\n",
    "    c_param_range = [0.01,0.1,1,10,100]\n",
    "    results_table = pd.DataFrame(index = range(len(c_param_range),2), columns = ['C_parameter','Mean recall score'])\n",
    "    results_table['C_parameter'] = c_param_range\n",
    "    \n",
    "    j = 0\n",
    "    for c_param in c_param_range:\n",
    "        print('C parameter: ', c_param)\n",
    "        recall_accs = []\n",
    "        for train_index, test_index in cv.split(x_train_data):\n",
    "            print(train_index)\n",
    "            print(test_index)\n",
    "            X_train, X_test = x_train_data.iloc[train_index], x_train_data.iloc[test_index]\n",
    "            Y_train, Y_test = y_train_data.iloc[train_index], y_train_data.iloc[test_index]\n",
    "            print(X_train.shape)\n",
    "            print(Y_train.shape)\n",
    "            lr = LogisticRegression(C = c_param, penalty = 'l2')\n",
    "            lr.fit(X_train, Y_train)\n",
    "            y_pred_undersample = lr.predict(X_test)\n",
    "            recall_acc = recall_score(Y_test,y_pred_undersample)\n",
    "            recall_accs.append(recall_acc)\n",
    "        results_table.iloc[j,'Mean recall score'] = np.mean(recall_accs)\n",
    "        j += 1\n",
    "        print('')\n",
    "        print('Mean recall score ', np.mean(recall_accs))\n",
    "        print('')\n",
    "\n",
    "    best_c = results_table.loc[results_table['Mean recall score'].idxmax()]['C_parameter']\n",
    "    \n",
    "    # Finally, we can check which C parameter is the best amongst the chosen.\n",
    "    print('*********************************************************************************')\n",
    "    print('Best model to choose from cross validation is with C parameter = ', best_c)\n",
    "    print('*********************************************************************************')\n",
    "    \n",
    "    return best_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"We can see from the model output that our model starts to perform better with SMOTE\"\"\"\n",
    "\n",
    "print(classification_report(Y_test, pred_smote, digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"ROC-AUC Curve for the model\"\"\"\n",
    "\n",
    "from sklearn import metrics\n",
    "preds = pred_smote_prob[:,1]\n",
    "y_test = np.array(Y_test)\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_test, preds)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RANDOM FOREST MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "rf = RandomForestRegressor(random_state=7)\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 50 , stop = 150, num = 25)]   # returns evenly spaced 25 numbers\n",
    "# Number of features to consider at every split\n",
    "max_features = ['log2', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(5, 10, num = 5)]  # returns evenly spaced numbers can be changed to any\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2,3,4,5,6,7,8,9,10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 3, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "learning_rate = [float(x) for x in np.linspace(0.1, 1, num = 10)]\n",
    "\n",
    "# Create the random grid\n",
    "params_r = {'n_estimators': n_estimators,\n",
    "            'max_features': max_features,\n",
    "            'max_depth': max_depth,\n",
    "            'min_samples_split': min_samples_split,\n",
    "            'min_samples_leaf': min_samples_leaf,\n",
    "            'learning_rate':learning_rate}\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "random = RandomizedSearchCV(estimator=rf, param_distributions=params_r,cv = k, random_state=7)\n",
    "\n",
    "# Fit the random search model\n",
    "random.fit(xtrain, ytrain)\n",
    "random.best_params_\n",
    "\n",
    "print(random.score(xtrain,ytrain))\n",
    "print(random.score(xtest,ytest))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
